{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2dba5f",
   "metadata": {},
   "source": [
    "***GENERATED CODE FOR dp PIPELINE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60034c3",
   "metadata": {},
   "source": [
    "***DON'T EDIT THIS CODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa88440",
   "metadata": {},
   "source": [
    "***CONNECTOR FUNCTIONS TO READ DATA.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "class HDFSConnector:\n",
    "\n",
    "    def fetch(spark, config):\n",
    "        ################### INPUT HADOOP HOST PORT TO CONNECT WITH ###############################\n",
    "        hdfs_server = str(os.environ['HDFS_SERVER'])\n",
    "        hdfs_port = int(os.environ['HDFS_PORT'])\n",
    "        df = spark.read.options(header='true', inferschema='true').csv(\n",
    "            f\"hdfs://{hdfs_server}:{hdfs_port}{eval(config)['url']}\", header='true')\n",
    "        display(df.limit(2).toPandas())\n",
    "        return df\n",
    "\n",
    "    def put(df, spark, config):\n",
    "        return df.write.format('csv').options(header='true' if eval(config)[\"is_header\"] == \"Use Header Line\" else 'false',\n",
    "                                              delimiter=eval(config)[\"delimiter\"]).save((\"%s %s\") % (datetime.datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")+\"_\", eval(config)['url']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c19572f",
   "metadata": {},
   "source": [
    "***OPERATION FUNCTIONS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from dask.dataframe import from_pandas\n",
    "import json\n",
    "\n",
    "\n",
    "def SortData(df, functionsData, applyOn=[]):\n",
    "    for functiondata in functionsData:\n",
    "        coldetail = functiondata['column']\n",
    "        sortdetails = functiondata['order']\n",
    "        for col in coldetail:\n",
    "            colname = col['columnName']\n",
    "    if sortdetails == \"asc\":\n",
    "        SortReq = True\n",
    "    else:\n",
    "        SortReq = False\n",
    "\n",
    "    df = df.set_index(colname)\n",
    "    #df = df.sort_values(by = colname, ascending=SortReq)\n",
    "\n",
    "    df = df.map_partitions(lambda df: df.sort_values(\n",
    "        [colname], ascending=SortReq)).reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def runDataCleansing(sparkDf, spark, config):\n",
    "    configObj = json.loads(config)\n",
    "    sparkDf.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "    df = from_pandas((sparkDf.toPandas()), npartitions=5)\n",
    "    functionList = configObj['functionsApplied']\n",
    "    Data_Cleansing_Methods = {\"replaceBy\": replaceValues,\n",
    "                              \"formula\": calculateFormula,\n",
    "                              \"aggregate\": aggregation,\n",
    "                              \"converttostringtype\": changeToString,\n",
    "                              \"editname\": renameColumns}\n",
    "    for function in functionList:\n",
    "        function['functionName']\n",
    "        df = Data_Cleansing_Methods[function['functionName']](df, function['functionsData'],\n",
    "                                                              function['applyOn'])\n",
    "    sparkDf = spark.createDataFrame(df.compute())\n",
    "\n",
    "    display(sparkDf.limit(2).toPandas())\n",
    "    return sparkDf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483de6b",
   "metadata": {},
   "source": [
    "***CONNECTOR FUNCTIONS TO WRITE DATA.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import datetime\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "class NumtraConnector:\n",
    "\n",
    "    def put(inStages, inStagesData, stageId, spark, config):\n",
    "        path = eval(config)['server_url']\n",
    "        baseType = eval(config)['baseType']\n",
    "        results_url = eval(config)['results_url']\n",
    "        server = eval(config)['server']\n",
    "        originalfile = eval(config)['orignalKey']\n",
    "        eval(config)['pathOnly']\n",
    "        filename = eval(config)['filename']\n",
    "        eval(config)['ser']\n",
    "        eval(config)['user']\n",
    "        eval(config)['password']\n",
    "        eval(config)['authSource']\n",
    "        eval(config)['user_id']\n",
    "        eval(config)['parent_id']\n",
    "        eval(config)['project_id']\n",
    "        time = str(int(datetime.datetime.now().timestamp()))\n",
    "\n",
    "        inStagesData[inStages[0]]\n",
    "\n",
    "        print(path)\n",
    "        print(baseType)\n",
    "        print(results_url)\n",
    "        print(server)\n",
    "        print(originalfile)\n",
    "        print(filename)\n",
    "\n",
    "        args = {\n",
    "            'url': path,\n",
    "            'baseType': baseType,\n",
    "            'originalfile': originalfile,\n",
    "            'filename': time + filename\n",
    "        }\n",
    "\n",
    "        response = requests.post(results_url, args)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e54d3e",
   "metadata": {},
   "source": [
    "***READING DATAFRAME***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72aee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CREATE SPARK SESSION ############################ ENTER YOUR SPARK MASTER IP AND PORT TO CONNECT TO SERVER ################\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[1]').getOrCreate()\n",
    "#%run dpHooks.ipynb\n",
    "try:\n",
    "\t#sourcePreExecutionHook()\n",
    "\n",
    "\tairpaengertrain = HDFSConnector.fetch(spark, \"{'url': '/FileStore/platform/testdata/1695810332888_AIRPassengerTrain.csv', 'filename': 'AIRPassengerTrain.csv', 'delimiter': ',', 'file_type': 'Delimeted', 'dbfs_token': '', 'dbfs_domain': '', 'FilePath': '/Retail And Marketing/Air Passenger/AIRPassengerTrain.csv', 'viewFileName': 'AIRPassengerTrain.csv', 'is_header': 'Use Header Line', 'baseType': 'hdfs', 'server_url': '/numtraPlatform/NumtraPlatformV3/uploads/platform/', 'results_url': 'http://ml.numtra.com:44040/api/read/hdfs'}\")\n",
    "\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n",
    "#spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ba3dd",
   "metadata": {},
   "source": [
    "***PERFORMING OPERATIONS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd89725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run dpHooks.ipynb\n",
    "try:\n",
    "\t#operationPreExecutionHook()\n",
    "\n",
    "datapreparation = runDataCleansing(airpaengertrain,spark,json.dumps( {\"url\": \"/FileStore/platform/testdata/1695810332888_AIRPassengerTrain.csv\", \"source_attributes\": {}, \"DataPrepFile\": \"/FileStore/platform/testdata/1695810332888_AIRPassengerTrain.csv\", \"data_source\": \"platfiles\", \"startListenerOnly\": 1, \"dateColumnNames\": [\"Month\"], \"FilePath\": \"/FileStore/platform/extra/658b23976c12e38b184d12801703618414/0part.csv\", \"requestRatio\": 0.0, \"totalRows\": 144, \"BasicStats\": {\"missingValues\": 0.0, \"numberOfColumns\": 2, \"numberOfRows\": 144, \"duplicateRowCount\": 0, \"stats\": [{\"column\": \"Month\", \"alias\": \"Month\", \"generated\": 0, \"type\": \"date\", \"max\": \"1960121\", \"min\": \"194911\", \"mean\": \"\", \"missing\": 0.0, \"stddev\": \"\", \"outliers\": [], \"validation\": []}, {\"column\": \"#Passengers\", \"alias\": \"#Passengers\", \"generated\": 0, \"type\": \"numeric\", \"max\": 622, \"min\": 104, \"mean\": 280.2986111111111, \"missing\": 0.0, \"stddev\": 119.97, \"outliers\": [606, 622], \"validation\": []}]}, \"predictionPowerScore\": [{\"#Passengers\": 1.0, \"Month\": 0.7245204046}, {\"#Passengers\": 0.0, \"Month\": 1.0}], \"HasBasicStats\": 1, \"functionsApplied\": [{\"functionName\": \"sortData\", \"applyOn\": [{\"columnName\": \"Month\", \"type\": \"date\", \"min\": \"194911\", \"max\": \"1960121\", \"mean\": \"-\"}], \"functionsData\": [{\"column\": [{\"columnName\": \"Month\", \"type\": \"date\", \"min\": \"194911\", \"max\": \"1960121\", \"mean\": \"-\"}], \"order\": \"desc\"}]}], \"functionChanges\": [{\"columnName\": \"Month\", \"functionName\": \"sortData\", \"Type\": \"date\", \"Parameters\": [{\"column\": [{\"columnName\": \"Month\", \"type\": \"date\", \"min\": \"194911\", \"max\": \"1960121\", \"mean\": \"-\"}], \"order\": \"desc\"}]}], \"fileheader\": [{\"field\": \"Month\", \"alias\": \"Month\", \"generated\": 0, \"position\": 1, \"type\": \"date\"}, {\"field\": \"#Passengers\", \"alias\": \"#Passengers\", \"generated\": 0, \"position\": 2, \"type\": \"numeric\"}]}))\n",
    "\t#operationPostExecutionHook(datapreparation)\n",
    "\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c0bdf",
   "metadata": {},
   "source": [
    "***WRITING DATAFRAME***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run dpHooks.ipynb\n",
    "try:\n",
    "\t#sinkPreExecutionHook()\n",
    "\n",
    "\ttestingproject = NumtraConnector.fetch(spark, \"{'samplefile': '/FileStore/platform/sampleData/658b23866c12e38b184d1262/part-00000-e9cc2987-098b-4a9e-9832-79b4d4ce060e-c000.csv', 'samplecount': 144, 'originalcount': 144, 'orignalKey': '/FileStore/platform/testdata/1695810332888_AIRPassengerTrain.csv', 'pathOnly': '/Testing Project', 'project_id': '6533f6e0490ac68cdae382f1', 'parent_id': '6533f6e0490ac68cdae382f1', 'original_schema': [{'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': 'Month', 'alias': 'Month', 'type': 'numeric', 'position': '0'}, {'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': '#Passengers', 'alias': '#Passengers', 'type': 'numeric', 'position': '1'}], 'actual_schema': [{'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': 'Month', 'alias': 'Month', 'type': 'numeric', 'position': '0'}, {'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': '#Passengers', 'alias': '#Passengers', 'type': 'numeric', 'position': '1'}], 'server': 'https://ml.numtra.com:443', 'server_url': '/numtraPlatform/NumtraPlatformV3/uploads/platform/', 'delimiter': ',', 'file_type': 'Delimeted', 'filename': 'airDP.csv', 'token': '', 'domain': '', 'is_header': 'Use Header Line', 'url': '/FileStore/platform/uploadedSourceFiles/part-00000-42f71402-429f-4e69-a43d-446b67b0dad9-c000.csv', 'results_url': 'http://ml.numtra.com:44040/api/read/hdfs'}\")\n",
    "\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
